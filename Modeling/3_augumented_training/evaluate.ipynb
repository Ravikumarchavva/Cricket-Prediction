{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "directory = r'D:\\github\\Cricket-Prediction\\data\\5_pytorchData'\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Augment the data by creating new samples with different combinations of overs\n",
    "def augment_data(team_stats_list, player_stats_list, ball_stats_list, over_segments=np.arange(7, 40)): \n",
    "    augmented_team_stats = []\n",
    "    augmented_player_stats = []\n",
    "    augmented_ball_stats = []\n",
    "    \n",
    "    for team_stats, player_stats, ball_stats in zip(team_stats_list, player_stats_list, ball_stats_list):\n",
    "        total_overs = ball_stats.shape[0] // 6  # Assuming 6 balls per over\n",
    "        for segment in over_segments:\n",
    "            if total_overs >= segment:\n",
    "                end_idx = segment * 6\n",
    "                augmented_team_stats.append(team_stats)\n",
    "                augmented_player_stats.append(player_stats)\n",
    "                augmented_ball_stats.append(ball_stats[:end_idx])\n",
    "    \n",
    "    return augmented_team_stats, augmented_player_stats, augmented_ball_stats\n",
    "\n",
    "# Create a custom Dataset\n",
    "class CricketDataset(Dataset):\n",
    "    def __init__(self, team_stats_list, player_stats_list, ball_stats_list):\n",
    "        self.team_stats_list = team_stats_list\n",
    "        self.player_stats_list = player_stats_list\n",
    "        self.ball_stats_list = ball_stats_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.team_stats_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        team_input = torch.tensor(self.team_stats_list[idx], dtype=torch.float32)\n",
    "        team_input = team_input.squeeze()  # Remove extra dimensions\n",
    "        player_input = torch.tensor(self.player_stats_list[idx], dtype=torch.float32)\n",
    "        ball_stats = torch.tensor(self.ball_stats_list[idx], dtype=torch.float32)\n",
    "        # Assuming the last column is the label\n",
    "        ball_input = ball_stats[:, :-1]\n",
    "        label = ball_stats[0, -1]\n",
    "        return team_input, player_input, ball_input, label\n",
    "\n",
    "# Define a collate function to handle variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    team_inputs = []\n",
    "    player_inputs = []\n",
    "    ball_inputs = []\n",
    "    labels = []\n",
    "    ball_lengths = []\n",
    "\n",
    "    for team_input, player_input, ball_input, label in batch:\n",
    "        team_inputs.append(team_input)\n",
    "        player_inputs.append(player_input)\n",
    "        ball_inputs.append(ball_input)\n",
    "        labels.append(label)\n",
    "        ball_lengths.append(ball_input.shape[0])\n",
    "\n",
    "    # Pad ball_inputs to the maximum sequence length in the batch\n",
    "    max_seq_len = max(ball_lengths)\n",
    "    padded_ball_inputs = torch.zeros(len(ball_inputs), max_seq_len, ball_inputs[0].shape[1])\n",
    "    for i, ball_input in enumerate(ball_inputs):\n",
    "        seq_len = ball_input.shape[0]\n",
    "        padded_ball_inputs[i, :seq_len, :] = ball_input\n",
    "\n",
    "    team_inputs = torch.stack(team_inputs)\n",
    "    player_inputs = torch.stack(player_inputs)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    return team_inputs, player_inputs, padded_ball_inputs, labels, ball_lengths\n",
    "\n",
    "\n",
    "test_dataloader = pickle.load(open(os.path.join(directory, 'test_dataloader.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the models\n",
    "class TeamStatsModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(TeamStatsModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class PlayerStatsModel(nn.Module):\n",
    "    def __init__(self, input_size, seq_len):\n",
    "        super(PlayerStatsModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=32, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(64 * ((seq_len - 4) // 4), 16)  # Adjust input size dynamically\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Convert to (batch, channels, seq_len)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return x\n",
    "\n",
    "class BallToBallModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BallToBallModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, 128, batch_first=True, bidirectional=False)  # Not bidirectional\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(128, 16)  # Adjust input size to 128\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the sequences\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        output_packed, (hn, cn) = self.lstm(x_packed)\n",
    "        # Use the final hidden state directly\n",
    "        hn = hn[-1,:,:]\n",
    "        x = self.dropout(hn)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return x\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, team_input_size, player_input_size, player_seq_len, ball_input_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.team_model = TeamStatsModel(team_input_size)\n",
    "        self.player_model = PlayerStatsModel(player_input_size, player_seq_len)\n",
    "        self.ball_model = BallToBallModel(ball_input_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16+16+16, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, team_input, player_input, ball_input, ball_lengths):\n",
    "        team_output = self.team_model(team_input)\n",
    "        player_output = self.player_model(player_input)\n",
    "        ball_output = self.ball_model(ball_input, ball_lengths)\n",
    "        combined = torch.cat((team_output, player_output, ball_output), dim=1)\n",
    "        output = self.fc(combined)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0235, Test Accuracy: 0.9893\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model\n",
    "team_input_size = test_dataloader.dataset[0][0].shape[0]\n",
    "player_input_size = test_dataloader.dataset[0][1].shape[1]\n",
    "player_seq_len = test_dataloader.dataset[0][1].shape[0]  # Sequence length for player stats\n",
    "ball_input_dim = test_dataloader.dataset[0][2].shape[1]\n",
    "\n",
    "# Initialize the model\n",
    "model = CombinedModel(team_input_size, player_input_size, player_seq_len, ball_input_dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Load the t20i model weights\n",
    "model.load_state_dict(torch.load('../2_naivetraining/t20i.pth',weights_only=True))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "test_correct_predictions = 0\n",
    "test_total_predictions = 0\n",
    "test_running_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for team_input, player_input, ball_input, labels, ball_lengths in test_dataloader:\n",
    "        team_input, player_input, ball_input, labels = team_input.to(device), player_input.to(device), ball_input.to(device), labels.to(device)\n",
    "        outputs = model(team_input, player_input, ball_input, ball_lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = (outputs > 0.5).float()\n",
    "        test_correct_predictions += (predictions == labels).sum().item()\n",
    "        test_total_predictions += labels.size(0)\n",
    "\n",
    "test_avg_loss = test_running_loss / len(test_dataloader)\n",
    "test_accuracy = test_correct_predictions / test_total_predictions\n",
    "print(f\"Test Loss: {test_avg_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99      4478\n",
      "         1.0       1.00      0.98      0.99      4497\n",
      "\n",
      "    accuracy                           0.99      8975\n",
      "   macro avg       0.99      0.99      0.99      8975\n",
      "weighted avg       0.99      0.99      0.99      8975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Get the predictions and ground truth labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for team_input, player_input, ball_input, labels, ball_lengths in test_dataloader:\n",
    "        team_input, player_input, ball_input, labels = team_input.to(device), player_input.to(device), ball_input.to(device), labels.to(device)\n",
    "        outputs = model(team_input, player_input, ball_input, ball_lengths)\n",
    "        predictions = (outputs > 0.5).float()\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(all_labels, all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0236, Test Accuracy: 0.9889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99      4478\n",
      "         1.0       1.00      0.98      0.99      4497\n",
      "\n",
      "    accuracy                           0.99      8975\n",
      "   macro avg       0.99      0.99      0.99      8975\n",
      "weighted avg       0.99      0.99      0.99      8975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter test data to take only the first 10 overs\n",
    "test_dataloader.dataset.ball_stats_list = [ball_stats[:20] for ball_stats in test_dataloader.dataset.ball_stats_list]\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "test_correct_predictions = 0\n",
    "test_total_predictions = 0\n",
    "test_running_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for team_input, player_input, ball_input, labels, ball_lengths in test_dataloader:\n",
    "        team_input, player_input, ball_input, labels = team_input.to(device), player_input.to(device), ball_input.to(device), labels.to(device)\n",
    "        outputs = model(team_input, player_input, ball_input, ball_lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = (outputs > 0.5).float()\n",
    "        test_correct_predictions += (predictions == labels).sum().item()\n",
    "        test_total_predictions += labels.size(0)\n",
    "\n",
    "test_avg_loss = test_running_loss / len(test_dataloader)\n",
    "test_accuracy = test_correct_predictions / test_total_predictions\n",
    "print(f\"Test Loss: {test_avg_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Get the predictions and ground truth labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for team_input, player_input, ball_input, labels, ball_lengths in test_dataloader:\n",
    "        team_input, player_input, ball_input, labels = team_input.to(device), player_input.to(device), ball_input.to(device), labels.to(device)\n",
    "        outputs = model(team_input, player_input, ball_input, ball_lengths)\n",
    "        predictions = (outputs > 0.5).float()\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(all_labels, all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
