{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player profile URL: https://www.espncricinfo.comhttps://www.espncricinfo.com/cricketers/parvez-rasool-378496\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Replace spaces in the player's name with '%20' (URL-encoded space)\n",
    "player_name = \"Parvez Rasool\"\n",
    "search_name = player_name.replace(\" \", \"%20\")\n",
    "\n",
    "# Set headers to mimic a real browser request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Step 1: Search for the player on ESPN Cricinfo\n",
    "search_url = f\"https://search.espncricinfo.com/ci/content/site/search.html?search={search_name}\"\n",
    "response = requests.get(search_url, headers=headers)\n",
    "\n",
    "# Step 2: Parse the search results page to find the first player's profile link\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all links in the search results that point to a cricketer's profile\n",
    "# These links typically contain \"/cricketers/\" in the URL\n",
    "player_links = soup.find_all(\"a\", href=True)\n",
    "for link in player_links:\n",
    "    if \"/cricketers/\" in link[\"href\"]:\n",
    "        profile_url = f\"https://www.espncricinfo.com{link['href']}\"\n",
    "        print(f\"Player profile URL: {profile_url}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"No cricketer profile found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player profiles and IDs have been added to the CSV.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chavv\\anaconda\\envs\\scrappper\\lib\\functools.py:888: DataOrientationWarning: Row orientation inferred during DataFrame construction. Explicitly specify the orientation by passing `orient=\"row\"` to silence this warning.\n",
      "  return dispatch(args[0].__class__)(*args, **kw)\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to extract player profile URL and ID\n",
    "def get_player_profile(player_name):\n",
    "    search_name = player_name.replace(\" \", \"%20\")  # URL-encode the player name\n",
    "    search_url = f\"https://search.espncricinfo.com/ci/content/site/search.html?search={search_name}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find the first player profile link\n",
    "    player_links = soup.find_all(\"a\", href=True)\n",
    "    for link in player_links:\n",
    "        if \"/cricketers/\" in link[\"href\"]:\n",
    "            profile_url = f\"https://www.espncricinfo.com{link['href']}\"\n",
    "            player_id = profile_url.split(\"-\")[-1]  # Extract player ID from URL\n",
    "            return profile_url, player_id\n",
    "    return None, None\n",
    "\n",
    "# Load the CSV with player names into a Polars DataFrame\n",
    "df = pl.read_csv(\"player_names.csv\")  # Assuming the CSV has a 'Player' column\n",
    "\n",
    "# Add new columns for profile URL and player ID\n",
    "df = df.with_columns([\n",
    "    pl.lit(\"\").alias(\"profile_url\"),\n",
    "    pl.lit(\"\").alias(\"player_id\")\n",
    "])\n",
    "\n",
    "# List to hold the updated data\n",
    "updated_data = []\n",
    "\n",
    "# Iterate over the DataFrame and search for each player\n",
    "for row in df.rows(named=True):\n",
    "    player_name = row['Player']\n",
    "    profile_url, player_id = get_player_profile(player_name)\n",
    "    \n",
    "    if profile_url and player_id:\n",
    "        updated_data.append((player_name, profile_url, player_id))\n",
    "    else:\n",
    "        updated_data.append((player_name, \"Not found\", \"Not found\"))\n",
    "\n",
    "# Convert the updated data back to a Polars DataFrame\n",
    "updated_df = pl.DataFrame(updated_data, schema=[\"Player\", \"profile_url\", \"player_id\"])\n",
    "\n",
    "# Save the updated DataFrame back to a CSV file\n",
    "updated_df.write_csv(\"players_with_ids.csv\")\n",
    "\n",
    "print(\"Player profiles and IDs have been added to the CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrappper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
