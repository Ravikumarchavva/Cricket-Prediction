{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-11-15T21:27:35.540+0530\u001b[0m] {\u001b[34mbase.py:\u001b[0m84} INFO\u001b[0m - Retrieving connection 'webhdfs_default'\u001b[0m\n",
      "[\u001b[34m2024-11-15T21:27:35.543+0530\u001b[0m] {\u001b[34mwebhdfs.py:\u001b[0m82} INFO\u001b[0m - Trying to connect to 192.168.245.142:9870\u001b[0m\n",
      "[\u001b[34m2024-11-15T21:27:35.546+0530\u001b[0m] {\u001b[34mwebhdfs.py:\u001b[0m86} INFO\u001b[0m - Trying namenode 192.168.245.142\u001b[0m\n",
      "[\u001b[34m2024-11-15T21:27:35.548+0530\u001b[0m] {\u001b[34mclient.py:\u001b[0m192} INFO\u001b[0m - Instantiated <InsecureClient(url='http://192.168.245.142:9870/')>.\u001b[0m\n",
      "[\u001b[34m2024-11-15T21:27:35.550+0530\u001b[0m] {\u001b[34mclient.py:\u001b[0m320} INFO\u001b[0m - Fetching status for '/'.\u001b[0m\n",
      "[\u001b[34m2024-11-15T21:27:35.560+0530\u001b[0m] {\u001b[34mwebhdfs.py:\u001b[0m96} INFO\u001b[0m - Using namenode 192.168.245.142 for hook\u001b[0m\n",
      "[\u001b[34m2024-11-15T21:27:35.562+0530\u001b[0m] {\u001b[34mclient.py:\u001b[0m724} INFO\u001b[0m - Reading file '/usr/ravi/t20/data/4_filteredData/ball_to_ball.csv'.\u001b[0m\n"
     ]
    },
    {
     "ename": "HdfsError",
     "evalue": "Path is not a file: /usr/ravi/t20/data/4_filteredData/ball_to_ball.csv\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:90)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2124)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:770)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:460)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHdfsError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpolars\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m     10\u001b[0m client \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_hdfs_client()\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m client\u001b[38;5;241m.\u001b[39mread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mFILTERED_DATA_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mball_to_ball.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[1;32m     13\u001b[0m     df \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mread_csv(reader)\n",
      "File \u001b[0;32m~/miniconda3/envs/t20i/lib/python3.12/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/t20i/lib/python3.12/site-packages/hdfs/client.py:725\u001b[0m, in \u001b[0;36mClient.read\u001b[0;34m(self, hdfs_path, offset, length, buffer_size, encoding, chunk_size, delimiter, progress)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDelimiter splitting incompatible with chunk size.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    724\u001b[0m _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, hdfs_path)\n\u001b[0;32m--> 725\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m  \u001b[49m\u001b[43mhdfs_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m  \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbuffersize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk_size \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m delimiter:\n",
      "File \u001b[0;32m~/miniconda3/envs/t20i/lib/python3.12/site-packages/hdfs/client.py:118\u001b[0m, in \u001b[0;36m_Request.to_method.<locals>.api_handler\u001b[0;34m(client, hdfs_path, data, strict, **params)\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m err\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetriableException\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStandbyException\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m strict:\n\u001b[0;32m--> 118\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m    121\u001b[0m attempted_hosts\u001b[38;5;241m.\u001b[39madd(host)\n",
      "\u001b[0;31mHdfsError\u001b[0m: Path is not a file: /usr/ravi/t20/data/4_filteredData/ball_to_ball.csv\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:90)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2124)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:770)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:460)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "import config, utils\n",
    "\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "client = utils.get_hdfs_client()\n",
    "\n",
    "with client.read(os.path.join(config.FILTERED_DATA_DIR, \"ball_to_ball.csv\")) as reader:\n",
    "    df = pl.read_csv(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525498, 2270, 49940, 2270, 2270, 2270)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balltoball.count(),balltoball[[\"match_id\",\"flip\"]].distinct().count(),player_stats.count(),player_stats[[\"match_id\",\"flip\"]].distinct().count(),team12_stats.count(),team12_stats[[\"match_id\",\"flip\"]].distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 0.0, 16.6, 0.0, 1, 1, 1, 1.0, 22.75, 8.38] [1, 2, 40.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0] [1, 0.1, 3, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(list(team12_stats_list[30][0].values())[2:], list(player_stats_list[30][0].values())[2:], list(balltoball_list[30][0].values())[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# list(team12_stats_list[30][0].values())[2:]\n",
    "# Create a custom Dataset\n",
    "class CricketDataset(Dataset):\n",
    "    def __init__(self, team_stats_list, player_stats_list, ball_stats_list, labels):\n",
    "        self.team_stats_list = team_stats_list\n",
    "        self.player_stats_list = player_stats_list\n",
    "        self.ball_stats_list = ball_stats_list\n",
    "        self.labels = labels\n",
    "        self.len = len(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        team_input = torch.tensor(self.team_stats_list[index], dtype=torch.float32)\n",
    "        team_input = team_input.squeeze()\n",
    "        player_input = torch.tensor(self.player_stats_list[index], dtype=torch.float32)\n",
    "        ball_stats = torch.tensor(self.ball_stats_list[index], dtype=torch.float32)\n",
    "        ball_input = ball_stats\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.float32)\n",
    "        return team_input, player_input, ball_input, label\n",
    "    \n",
    "# Define a collate function to handle variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    team_inputs = []\n",
    "    player_inputs = []\n",
    "    ball_inputs = []\n",
    "    labels = []\n",
    "    ball_lengths = []\n",
    "\n",
    "    for team_input, player_input, ball_input, label in batch:\n",
    "        team_inputs.append(team_input)\n",
    "        player_inputs.append(player_input)\n",
    "        ball_inputs.append(ball_input)\n",
    "        labels.append(label)\n",
    "        ball_lengths.append(ball_input.shape[0])\n",
    "\n",
    "    max_seq_len = max(ball_lengths)\n",
    "    padded_ball_inputs = torch.zeros(len(ball_inputs), max_seq_len, ball_inputs[0].shape[1])\n",
    "    for i, ball_input in enumerate(ball_inputs):\n",
    "        seq_len = ball_input.shape[0]\n",
    "        padded_ball_inputs[i, :seq_len, :] = ball_input\n",
    "\n",
    "    team_inputs = torch.stack(team_inputs)\n",
    "    player_inputs = torch.stack(player_inputs)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    return team_inputs, player_inputs, padded_ball_inputs, labels, ball_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t20i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
