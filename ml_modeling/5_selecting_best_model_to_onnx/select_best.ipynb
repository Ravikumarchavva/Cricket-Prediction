{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run ID: yjilby5q\n",
      "Validation Accuracy: 83.49514563106796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lr': 0.0001,\n",
       " 'dropout': 0.6882847989323215,\n",
       " 'batch_size': 32,\n",
       " 'num_epochs': 100,\n",
       " 'num_layers': 3,\n",
       " 'hidden_size': 256,\n",
       " 'enable_plots': False,\n",
       " 'weight_decay': 5.754095511533712e-06,\n",
       " 'learning_rate': 0.0009693209823947022}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize your WandB API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Define your project and sweep details\n",
    "entity = \"ravikumarchavva-org\"  # Replace with your WandB organization or username\n",
    "project = \"T20I-CRICKET-WINNER-PREDICTION\"\n",
    "sweep_id = \"qqakx1g3\"  # The specific sweep ID\n",
    "\n",
    "# Fetch the sweep\n",
    "sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "\n",
    "# Retrieve all runs in the sweep\n",
    "runs = sweep.runs\n",
    "\n",
    "# Sort runs by a specific metric, e.g., validation accuracy (replace with your metric name)\n",
    "# Use the metric key name you logged in your WandB runs\n",
    "best_run = sorted(\n",
    "    runs, key=lambda run: run.summary.get(\"val_accuracy\", float(\"-inf\")), reverse=True\n",
    ")[1]\n",
    "\n",
    "# Print details of the best run\n",
    "print(f\"Best run ID: {best_run.id}\")\n",
    "print(f\"Validation Accuracy: {best_run.summary.get('val_accuracy')}\")\n",
    "config = best_run.config\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading artifact: best_model_val_loss_0.3524:v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact downloaded to: d:\\github\\Cricket-Prediction\\ml_modeling\\5_selecting_best_model\\artifacts\\best_model_val_loss_0.3524-v0\n"
     ]
    }
   ],
   "source": [
    "# Get the model from run id\n",
    "run_path = f\"ravikumarchavva-org/T20I-CRICKET-WINNER-PREDICTION/{best_run.id}\"\n",
    "\n",
    "# Get the specific run\n",
    "run = api.run(run_path)\n",
    "\n",
    "# List and download output artifacts\n",
    "for artifact in run.logged_artifacts():\n",
    "    if artifact.name.startswith(\"best_model\"):\n",
    "        print(f\"Downloading artifact: {artifact.name}\")\n",
    "        artifact_dir = artifact.download()\n",
    "        print(f\"Artifact downloaded to: {artifact_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: ravikumarchavva (ravikumarchavva-org). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\github\\Cricket-Prediction\\ml_modeling\\5_selecting_best_model\\wandb\\run-20241127_144728-ojqqzhrf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ravikumarchavva-org/T20I-CRICKET-WINNER-PREDICTION/runs/ojqqzhrf' target=\"_blank\">lunar-thunder-242</a></strong> to <a href='https://wandb.ai/ravikumarchavva-org/T20I-CRICKET-WINNER-PREDICTION' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ravikumarchavva-org/T20I-CRICKET-WINNER-PREDICTION' target=\"_blank\">https://wandb.ai/ravikumarchavva-org/T20I-CRICKET-WINNER-PREDICTION</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ravikumarchavva-org/T20I-CRICKET-WINNER-PREDICTION/runs/ojqqzhrf' target=\"_blank\">https://wandb.ai/ravikumarchavva-org/T20I-CRICKET-WINNER-PREDICTION/runs/ojqqzhrf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\",\"..\"))\n",
    "\n",
    "from utils.data_utils import collate_fn_with_padding, load_datasets, augument_data\n",
    "from utils.model_utils import set_seed\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "set_seed()\n",
    "# Load the Datasets\n",
    "train_dataset, val_dataset, test_dataset = load_datasets()\n",
    "\n",
    "# Step 2: Augment Data\n",
    "train_dataset, val_dataset, test_dataset = augument_data(train_dataset, val_dataset, test_dataset)\n",
    "\n",
    "# Step 3: Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=collate_fn_with_padding)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn_with_padding)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn_with_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import os\n",
    "\n",
    "# Load the model\n",
    "model = torch.load(os.path.join(artifact_dir, 'best_model.pth'),weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on `test_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.58 %\n",
      "\n",
      "Test Data Metrics:\n",
      "Overall Metrics:\n",
      "  Stage  accuracy  precision   recall      f1\n",
      "Overall  0.855769   0.924138 0.797619 0.85623\n",
      "\n",
      "Stage Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stage</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 overs</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 overs</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.754717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15 overs</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.808989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20 overs</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25 overs</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30 overs</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.865979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35 overs</td>\n",
       "      <td>0.861905</td>\n",
       "      <td>0.925234</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.872247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40 overs</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.886364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45 overs</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.884211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Stage  accuracy  precision    recall        f1\n",
       "0   5 overs  0.700000   0.666667  0.500000  0.571429\n",
       "1  10 overs  0.783333   0.869565  0.666667  0.754717\n",
       "2  15 overs  0.811111   0.878049  0.750000  0.808989\n",
       "3  20 overs  0.850000   0.900000  0.818182  0.857143\n",
       "4  25 overs  0.860000   0.909091  0.833333  0.869565\n",
       "5  30 overs  0.855556   0.913043  0.823529  0.865979\n",
       "6  35 overs  0.861905   0.925234  0.825000  0.872247\n",
       "7  40 overs  0.875000   0.928571  0.847826  0.886364\n",
       "8  45 overs  0.877778   0.933333  0.840000  0.884211"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.model_utils import evaluate_model\n",
    "\n",
    "config['enable_plots'] = False\n",
    "\n",
    "save_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Define window sizes\n",
    "window_sizes = [5,10,15,20, 25, 30, 35, 40, 45]\n",
    "\n",
    "# Evaluate the model\n",
    "metrics, all_labels, all_predictions, all_probs = evaluate_model(\n",
    "    model, test_dataloader, device, window_sizes, config, save_dir=os.getcwd()\n",
    ")\n",
    "overall_metrics = metrics[\"overall_metrics\"]\n",
    "stage_metrics = metrics[\"stage_metrics\"]\n",
    "import pandas as pd\n",
    "# Convert metrics to pandas DataFrames\n",
    "stage_df = pd.DataFrame(stage_metrics).T\n",
    "stage_df.index.name = \"Stage\"\n",
    "stage_df.reset_index(inplace=True)\n",
    "\n",
    "overall_df = pd.DataFrame(overall_metrics, index=[\"Overall\"]).reset_index()\n",
    "overall_df.rename(columns={\"index\": \"Stage\"}, inplace=True)\n",
    "\n",
    "print(\"\\nTest Data Metrics:\")\n",
    "# Print metrics in DataFrame format\n",
    "print(\"Overall Metrics:\")\n",
    "print(overall_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nStage Metrics:\")\n",
    "stage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 13]), torch.Size([1, 1, 22, 12]), torch.Size([1, 120, 10]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i = torch.randint(0, len(test_dataset), (1,)).item()\n",
    "i = 0\n",
    "train_dataset[i][0].unsqueeze(0).shape, train_dataset[i][1].unsqueeze(0).unsqueeze(0).shape, train_dataset[i][2].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[i][0].shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_visualization.png'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Visualize the model architecture\n",
    "from torchinfo import summary  # Replace torchsummary with torchinfo\n",
    "from torchviz import make_dot  # Add import for torchviz\n",
    "\n",
    "# Visualize the model architecture using torchinfo\n",
    "summary(model, input_size=[(1, 13), (1, 1, 22, 12), (1, 10)])\n",
    "\n",
    "# Create a dummy input to visualize the graph\n",
    "team_dummy = train_dataset[i][0].unsqueeze(0).to(device)\n",
    "player_dummy = train_dataset[i][1].unsqueeze(0).to(device)\n",
    "ball_dummy = train_dataset[i][2].unsqueeze(0).to(device)\n",
    "# Forward pass to get the output\n",
    "output = model(team_dummy, player_dummy, ball_dummy)\n",
    "\n",
    "# Generate and save the model visualization\n",
    "dot = make_dot(output, params=None)\n",
    "dot.format = 'png'\n",
    "dot.render('model_visualization')  # Saves as model_visualization.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.data_utils.CricketDataset at 0x1f284046160>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:4545: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been exported to d:\\github\\Cricket-Prediction\\ml_modeling\\5_selecting_best_model\\model.onnx\n"
     ]
    }
   ],
   "source": [
    "# Export the model to ONNX\n",
    "\n",
    "from utils.model_utils import export_model_to_onnx\n",
    "\n",
    "export_path = os.path.join(os.getcwd(), \"model.onnx\")\n",
    "export_model_to_onnx(model, export_path, (team_dummy, player_dummy, ball_dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Names: ['team_input', 'player_input', 'ball_input']\n",
      "Output Names: ['output']\n"
     ]
    }
   ],
   "source": [
    "#import from onnx\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(export_path)\n",
    "\n",
    "# Check the ONNX model\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# Initialize the ONNX runtime session\n",
    "ort_session = ort.InferenceSession(export_path)\n",
    "\n",
    "# Get the input names\n",
    "input_names = [input.name for input in ort_session.get_inputs()]\n",
    "\n",
    "# Get the output names\n",
    "output_names = [output.name for output in ort_session.get_outputs()]\n",
    "\n",
    "# Print the input and output names\n",
    "print(f\"Input Names: {input_names}\")\n",
    "print(f\"Output Names: {output_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i= torch.randint(0, len(val_dataset), (1,)).item()\n",
    "team_input, player_input, ball_input,label = train_dataset[i]\n",
    "team_input = team_input.unsqueeze(0).to(device)\n",
    "player_input = player_input.unsqueeze(0).to(device)\n",
    "ball_input = ball_input.unsqueeze(0).to(device)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_input = {\n",
    "    \"team_input\": team_input.cpu().numpy(),\n",
    "    \"player_input\": player_input.cpu().numpy(),\n",
    "    \"ball_input\": ball_input.cpu().numpy(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.9979929]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Run the ONNX model using the ONNX runtime session\n",
    "outputs = ort_session.run(None, onnx_input)\n",
    "\n",
    "# Print the outputs\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
