{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run ID: yjilby5q\n",
      "Validation Accuracy: 83.49514563106796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lr': 0.0001,\n",
       " 'dropout': 0.6882847989323215,\n",
       " 'batch_size': 32,\n",
       " 'num_epochs': 100,\n",
       " 'num_layers': 3,\n",
       " 'hidden_size': 256,\n",
       " 'enable_plots': False,\n",
       " 'weight_decay': 5.754095511533712e-06,\n",
       " 'learning_rate': 0.0009693209823947022}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize your WandB API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Define your project and sweep details\n",
    "entity = \"ravikumarchavva-org\"  # Replace with your WandB organization or username\n",
    "project = \"T20I-CRICKET-WINNER-PREDICTION\"\n",
    "sweep_id = \"qqakx1g3\"  # The specific sweep ID\n",
    "\n",
    "# Fetch the sweep\n",
    "sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "\n",
    "# Retrieve all runs in the sweep\n",
    "runs = sweep.runs\n",
    "\n",
    "# Sort runs by a specific metric, e.g., validation accuracy (replace with your metric name)\n",
    "# Use the metric key name you logged in your WandB runs\n",
    "best_run = sorted(\n",
    "    runs, key=lambda run: run.summary.get(\"val_accuracy\", float(\"-inf\")), reverse=True\n",
    ")[1]\n",
    "\n",
    "# Print details of the best run\n",
    "print(f\"Best run ID: {best_run.id}\")\n",
    "print(f\"Validation Accuracy: {best_run.summary.get('val_accuracy')}\")\n",
    "config = best_run.config\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading artifact: best_model_val_loss_0.3524:v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact downloaded to: d:\\github\\Cricket-Prediction\\ml_modeling\\5_selecting_best_model\\artifacts\\best_model_val_loss_0.3524-v0\n"
     ]
    }
   ],
   "source": [
    "# Get the model from run id\n",
    "run_path = f\"ravikumarchavva-org/T20I-CRICKET-WINNER-PREDICTION/{best_run.id}\"\n",
    "\n",
    "# Get the specific run\n",
    "run = api.run(run_path)\n",
    "\n",
    "# List and download output artifacts\n",
    "for artifact in run.logged_artifacts():\n",
    "    if artifact.name.startswith(\"best_model\"):\n",
    "        print(f\"Downloading artifact: {artifact.name}\")\n",
    "        artifact_dir = artifact.download()\n",
    "        print(f\"Artifact downloaded to: {artifact_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\",\"..\"))\n",
    "\n",
    "from utils.data_utils import collate_fn_with_padding, load_datasets, augument_data\n",
    "from utils.model_utils import set_seed\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "set_seed()\n",
    "# Load the Datasets\n",
    "train_dataset, val_dataset, test_dataset = load_datasets()\n",
    "\n",
    "# Step 2: Augment Data\n",
    "train_dataset, val_dataset, test_dataset = augument_data(train_dataset, val_dataset, test_dataset)\n",
    "\n",
    "# Step 3: Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=collate_fn_with_padding)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn_with_padding)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn_with_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import os\n",
    "\n",
    "# Load the model\n",
    "model = torch.load(os.path.join(artifact_dir, 'best_model.pth'),weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on `test_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.58 %\n",
      "\n",
      "Test Data Metrics:\n",
      "Overall Metrics:\n",
      "  Stage  accuracy  precision   recall      f1\n",
      "Overall  0.855769   0.924138 0.797619 0.85623\n",
      "\n",
      "Stage Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stage</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 overs</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 overs</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.754717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15 overs</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.808989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20 overs</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25 overs</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30 overs</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.865979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35 overs</td>\n",
       "      <td>0.861905</td>\n",
       "      <td>0.925234</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.872247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40 overs</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.886364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45 overs</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.884211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Stage  accuracy  precision    recall        f1\n",
       "0   5 overs  0.700000   0.666667  0.500000  0.571429\n",
       "1  10 overs  0.783333   0.869565  0.666667  0.754717\n",
       "2  15 overs  0.811111   0.878049  0.750000  0.808989\n",
       "3  20 overs  0.850000   0.900000  0.818182  0.857143\n",
       "4  25 overs  0.860000   0.909091  0.833333  0.869565\n",
       "5  30 overs  0.855556   0.913043  0.823529  0.865979\n",
       "6  35 overs  0.861905   0.925234  0.825000  0.872247\n",
       "7  40 overs  0.875000   0.928571  0.847826  0.886364\n",
       "8  45 overs  0.877778   0.933333  0.840000  0.884211"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.model_utils import evaluate_model\n",
    "\n",
    "config['enable_plots'] = False\n",
    "\n",
    "save_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Define window sizes\n",
    "window_sizes = [5,10,15,20, 25, 30, 35, 40, 45]\n",
    "\n",
    "# Evaluate the model\n",
    "metrics, all_labels, all_predictions, all_probs = evaluate_model(\n",
    "    model, test_dataloader, device, window_sizes, config, save_dir=os.getcwd()\n",
    ")\n",
    "overall_metrics = metrics[\"overall_metrics\"]\n",
    "stage_metrics = metrics[\"stage_metrics\"]\n",
    "import pandas as pd\n",
    "# Convert metrics to pandas DataFrames\n",
    "stage_df = pd.DataFrame(stage_metrics).T\n",
    "stage_df.index.name = \"Stage\"\n",
    "stage_df.reset_index(inplace=True)\n",
    "\n",
    "overall_df = pd.DataFrame(overall_metrics, index=[\"Overall\"]).reset_index()\n",
    "overall_df.rename(columns={\"index\": \"Stage\"}, inplace=True)\n",
    "\n",
    "print(\"\\nTest Data Metrics:\")\n",
    "# Print metrics in DataFrame format\n",
    "print(\"Overall Metrics:\")\n",
    "print(overall_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nStage Metrics:\")\n",
    "stage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 13]), torch.Size([1, 1, 22, 12]), 1, 10)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.randint(0, len(test_dataset), (1,)).item()\n",
    "train_dataset[i][0].unsqueeze(0).shape, train_dataset[i][1].unsqueeze(0).unsqueeze(0).shape, train_dataset[i][2].unsqueeze(0).shape[0], train_dataset[i][2].unsqueeze(0).shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[i][0].shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_visualization.png'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Visualize the model architecture\n",
    "from torchinfo import summary  # Replace torchsummary with torchinfo\n",
    "from torchviz import make_dot  # Add import for torchviz\n",
    "\n",
    "# Visualize the model architecture using torchinfo\n",
    "summary(model, input_size=[(1, 13), (1, 1, 22, 12), (1, 10)])\n",
    "\n",
    "# Create a dummy input to visualize the graph\n",
    "team_dummy = torch.randn(1, 13).to(device)\n",
    "player_dummy = torch.randn(1, 1, 22, 12).to(device)\n",
    "ball_dummy = torch.randn(1, 10).to(device)\n",
    "# Forward pass to get the output\n",
    "output = model(team_dummy, player_dummy, ball_dummy)\n",
    "\n",
    "# Generate and save the model visualization\n",
    "dot = make_dot(output, params=None)\n",
    "dot.format = 'png'\n",
    "dot.render('model_visualization')  # Saves as model_visualization.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'export_model_to_onnx' from 'utils.model_utils' (d:\\github\\Cricket-Prediction\\ml_modeling\\5_selecting_best_model\\..\\..\\utils\\model_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m export_model_to_onnx\n\u001b[0;32m      6\u001b[0m export_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.onnx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m export_model_to_onnx(model, export_path, (team_dummy, player_dummy, ball_dummy))\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'export_model_to_onnx' from 'utils.model_utils' (d:\\github\\Cricket-Prediction\\ml_modeling\\5_selecting_best_model\\..\\..\\utils\\model_utils.py)"
     ]
    }
   ],
   "source": [
    "# Export the model to ONNX\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "from utils.model_utils import export_model_to_onnx\n",
    "export_path = os.path.join(os.getcwd(), 'model.onnx')\n",
    "export_model_to_onnx(model, export_path, (team_dummy, player_dummy, ball_dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
